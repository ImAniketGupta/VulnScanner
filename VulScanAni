import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
from fpdf import FPDF
from zapv2 import ZAPv2  # Import ZAPv2 for using OWASP ZAP API
import unicodedata

class WebCrawler:
    def __init__(self, base_url=None, max_urls=50):
        self.base_url = base_url
        self.max_urls = max_urls
        self.visited_urls = set()

    def crawl(self, url):
        if len(self.visited_urls) >= self.max_urls:
            return
        
        url = urljoin(self.base_url, url)
        parsed_url = urlparse(url)
        
        if parsed_url.netloc != urlparse(self.base_url).netloc or url in self.visited_urls:
            return
        
        print(f"Visiting: {url}")
        self.visited_urls.add(url)

        try:
            response = requests.get(url)
            response.raise_for_status()
        except (requests.HTTPError, requests.RequestException) as e:
            print(f"Failed to access {url}: {e}")
            return

        soup = BeautifulSoup(response.text, "html.parser")
        for link in soup.find_all("a", href=True):
            if len(self.visited_urls) >= self.max_urls:
                return
            href = link.get("href")
            full_url = urljoin(url, href)
            if full_url not in self.visited_urls:
                time.sleep(0.5)
                self.crawl(full_url)

    def start_crawling(self):
        if self.base_url:
            self.crawl(self.base_url)
        print("\nCrawling completed. Visited URLs:")
        for url in self.visited_urls:
            print(url)
        return list(self.visited_urls)

class AGVulScanner:
    def __init__(self, urls, app_name):
        self.urls = urls
        self.app_name = app_name  # Save the application name for the report file
        # Initialize ZAP 
        self.zap = ZAPv2(proxies={'http': 'http://localhost:8080', 'https': 'http://localhost:8080'})
        self.report_data = {}
        self.vulnerability_counts = {}

    def test_connection(self):
        try:
            # Test connection to ZAP
            version = self.zap.core.version()
            print("Connected to OWASP ZAP version:", version)
        except Exception as e:
            print("Error connecting to OWASP ZAP:", e)

    def run_zap_scan(self, target, scan_timeout=300):
        print(f"\n[+] Running OWASP ZAP scan on {target}...")
        self.zap.urlopen(target)
        time.sleep(2)

        # Start an active scan
        self.zap.ascan.scan(target)
        start_time = time.time()

        # Monitor scan progress
        while int(self.zap.ascan.status()) < 100:
            elapsed_time = time.time() - start_time
            if elapsed_time > scan_timeout:
                print(f"[-] ZAP scan timed out after {scan_timeout} seconds.")
                break
            print(f"Scan progress: {self.zap.ascan.status()}% - Elapsed time: {int(elapsed_time)} seconds")
            time.sleep(5)

        # Collect and save alerts if scan completes
        if int(self.zap.ascan.status()) == 100:
            alerts = self.zap.core.alerts(baseurl=target)
            self.report_data[target] = [
                {
                    "alert": alert['alert'],
                    "risk": alert['risk'],
                    "description": alert['description'],
                    "url": alert['url'],
                    "solution": alert['solution']
                } for alert in alerts
            ]
            print(f"[+] ZAP scan completed for {target}.")
            self.count_vulnerabilities(alerts)  # Update vulnerability counts
        else:
            print(f"[-] Scan did not complete for {target} due to timeout.")
    
    def count_vulnerabilities(self, alerts):
        # Count vulnerabilities based on type and severity
        for alert in alerts:
            alert_type = alert['alert']
            risk_level = alert['risk']

            # Initialize counters if they don't exist
            if alert_type not in self.vulnerability_counts:
                self.vulnerability_counts[alert_type] = {'Low': 0, 'Medium': 0, 'High': 0}

            if risk_level == 'Low':
                self.vulnerability_counts[alert_type]['Low'] += 1
            elif risk_level == 'Medium':
                self.vulnerability_counts[alert_type]['Medium'] += 1
            elif risk_level == 'High':
                self.vulnerability_counts[alert_type]['High'] += 1

    def generate_pdf_report(self):
    	# Save the report with the application name in the file name
        report_filename = f"{self.app_name}_vulnerability_report.pdf"
        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        
        # First Page: Large and bold application name
        pdf.add_page()
        pdf.set_font("Arial", "B", 20)
        pdf.cell(200, 10, f"{self.app_name} - Web Vulnerability Report", 0, 1, "C")
        pdf.set_font("Arial", "", 12)
        pdf.ln(10)

        for target, alerts in self.report_data.items():
            pdf.cell(200, 10, f"Target: {target}", 0, 1)
            pdf.line(10, pdf.get_y(), 200, pdf.get_y())
            pdf.ln(5)
            for alert in alerts:
                pdf.cell(0, 10, f"Alert: {alert['alert']}", 0, 1)
                pdf.cell(0, 10, f"Risk Level: {alert['risk']}", 0, 1)
                pdf.multi_cell(0, 8, f"Description: {self.prepare_text_for_pdf(alert)}")
                pdf.multi_cell(0, 8, f"URL: {alert['url']}")
                pdf.multi_cell(0, 8, f"Solution: {alert['solution']}")
                pdf.ln(10)
                
        # Start new page for vulnerability counts and summaries
        pdf.add_page()
        pdf.set_font("Arial", "B", 14)
        pdf.cell(200, 10, "Vulnerability Counts by Type and Severity", 0, 1, "C")
        pdf.ln(5)

        # Add vulnerability count data
        for alert_type, severity_counts in self.vulnerability_counts.items():
            pdf.cell(0, 10, f"Vulnerability Type: {alert_type}", 0, 1)
            for severity, count in severity_counts.items():
                pdf.cell(0, 10, f"{severity} Severity: {count}", 0, 1)
            pdf.ln(5)

        # Subsequent pages: Small application name in header and footer
        pdf.alias_nb_pages()
        pdf.add_page()
        pdf.set_font("Arial", "I", 10)  # Smaller font for header and footer
        pdf.cell(200, 10, f"{self.app_name}", 0, 1, "C")
        pdf.ln(5)
        
        pdf.output(report_filename)
        print(f"\nPDF report generated: {report_filename}")

    def prepare_text_for_pdf(self, data):
        # Convert text to a printable format for fpdf (handles Unicode issues)
        # This will replace unsupported characters
        safe_text = unicodedata.normalize('NFKD', data.get('description', ''))
        # Remove characters that cannot be encoded with 'latin1'
        safe_text = safe_text.encode('latin1', 'ignore').decode('latin1')
        
        # Optionally, replace problematic characters with something else
        safe_text = safe_text.replace("\u2014", "-")  # Replace em dash with regular dash
        return safe_text

    def start_scan(self):
        for target in self.urls:
            print(f"\nStarting scan for target: {target}")
            self.run_zap_scan(target)
        self.generate_pdf_report()

def main():
    app_name = input("Enter the application name: ")  # Ask for the application name
    print("Choose an option:")
    print("1. Enter a base URL to crawl the entire site.")
    print("2. Provide a list of URLs to visit individually.")

    choice = input("Enter your choice (1 or 2): ")

    if choice == '1':
        base_url = input("Enter the base URL (e.g., https://example.com): ")
        max_urls = int(input("Enter the maximum number of URLs to visit: "))
        crawler = WebCrawler(base_url, max_urls)
        urls_to_scan = crawler.start_crawling()

    elif choice == '2':
        urls = input("Enter a list of URLs to visit, separated by commas:\n").split(',')
        max_urls = int(input("Enter the maximum number of URLs to visit: "))
        
        crawler = WebCrawler(max_urls=max_urls)
        urls_to_scan = []
        for url in urls:
            if len(crawler.visited_urls) >= max_urls:
                break
            url = url.strip()
            crawler.base_url = url
            urls_to_scan.extend(crawler.start_crawling())
    else:
        print("Invalid choice. Please enter 1 or 2.")
        return

    # Run the vulnerability scanner on the gathered URLs
    scanner = AGVulScanner(urls_to_scan, app_name)
    scanner.start_scan()

if __name__ == "__main__":
    main()
